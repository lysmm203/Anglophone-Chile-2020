{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modelling for Specific Articles Related to Cochrane",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8elswjH5WWda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a003504-7140-4c20-9019-e3568b1cca94"
      },
      "source": [
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric, stem_text\n",
        "from gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, strip_short\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.utils import simple_preprocess\n",
        "from smart_open import smart_open\n",
        "import csv\n",
        "from gensim.models import CoherenceModel\n",
        "import pandas as pd\n",
        "import os, json\n",
        "import time\n",
        "import ast\n",
        "\n",
        "import errno\n",
        "import glob\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "snowball = SnowballStemmer('english')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.3)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.17.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=8dc47e0ea46cc6414ab35aaac7941b5a0c49644a9c7a8a69daa83a293301beae\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.15 pyLDAvis-2.1.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXXujE20YKdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6d6747-953b-40d8-8f1b-92300d695a23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4AJUOU8WpMP"
      },
      "source": [
        "custom_filters = [lambda filename: filename.lower(), #lowercase\n",
        "                  strip_multiple_whitespaces, #remove repeating whitespaces\n",
        "                  strip_numeric, #remove numbers\n",
        "                  remove_stopwords, #remove stopwords\n",
        "                  strip_short, #remove words less than 3 characters long\n",
        "                  strip_punctuation #remove all punctuations \n",
        "                  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqlLAad-Wp1I"
      },
      "source": [
        "def lem_stem(word):\n",
        "  word = lemmatizer.lemmatize(word)\n",
        "  word = snowball.stem(word)\n",
        "  return word "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7pYHvotWrey"
      },
      "source": [
        "def gensim_process(json_file):\n",
        "\n",
        "  # Declaring global variable that can be used outside functions\n",
        "  global corpus\n",
        "  global dictionary\n",
        "\n",
        "  dataframe = pd.read_json(json_file, lines = True)\n",
        "  dataframe = dataframe.to_string()\n",
        "\n",
        "  # preprocess the text through the custom filter \n",
        "  gensim_processed = preprocess_string(dataframe, custom_filters) \n",
        "\n",
        "  # lemmatize and then stem the words\n",
        "  processed = ' '.join([lem_stem(word) for word in gensim_processed])\n",
        "\n",
        "  #filter again after lemma+stem  \n",
        "  final_processed = preprocess_string(processed, custom_filters)\n",
        "\n",
        "  # split the string into a list to enable the tokens to be converted into a dictionary \n",
        "  dataset = [d.split() for d in final_processed]\n",
        "\n",
        "\n",
        "  # convert the data into a dictionary to be able to process the data through gensim\n",
        "  dictionary = corpora.Dictionary(dataset)\n",
        "\n",
        "  # Convert document (a list of words) into the bag-of-words format\n",
        "  corpus = [dictionary.doc2bow(text) for text in dataset]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TMwvHoIWvE4"
      },
      "source": [
        "def visualize(corpus, dictionary, main, year):\n",
        "  start = time.time()\n",
        "\n",
        "  #initialize the tfidf model \n",
        "  tfidf = models.TfidfModel(corpus)\n",
        "\n",
        "  #apply the transofmration to the entire corpus \n",
        "  transformed_tfidtf = tfidf[corpus]\n",
        "\n",
        "  #use the LDA transformation on tfidf\n",
        "  lda = models.LdaMulticore(transformed_tfidtf, num_topics=8, id2word=dictionary)\n",
        "  lda.show_topics()\n",
        "\n",
        "  #visualizing the topics\n",
        "  pyLDAvis.enable_notebook()\n",
        "  vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
        "  print(time.time()- start)\n",
        "\n",
        "  #saving data\n",
        "  with open('/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/{}/vis_file_{}_{}.html'.format(main,year,main,year), 'w') as file: \n",
        "    pyLDAvis.save_html(vis, file)\n",
        "\n",
        "  csvfile = open('/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/{}/corpus_{}_{}.csv'.format(main,year,main,year), 'w')\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  for item in corpus:\n",
        "    csvwriter.writerow(item)\n",
        "  csvfile.close()\n",
        "\n",
        "  w = csv.writer(open('/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/{}/dictionary_{}_{}.csv'.format(main,year,main,year), \"w\"))\n",
        "  for key, val in dictionary.items():\n",
        "    w.writerow([key, val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y1jm9ZnWwsf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "9a32cce8-1a44-4fc9-9289-f247ffc38abf"
      },
      "source": [
        "# Asks the user for the name of the main folder and changes to its directory\n",
        "#main_folder = input('Enter the main folder to run: ')\n",
        "main_folder = input('Enter main folder to run: ')\n",
        "folder_path = \"/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}\".format(main_folder)\n",
        "\n",
        "\n",
        "years = [folder for folder in os.listdir(folder_path)]\n",
        "print (years)\n",
        "\n",
        "\n",
        "\n",
        "corpus_external = []\n",
        "dictionary_external = corpora.Dictionary()\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "for year in years:\n",
        "  year_path = \"/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/{}\".format(main_folder, year)\n",
        "  \n",
        "  json_files = [pos_json for pos_json in\n",
        "              os.listdir(year_path) if pos_json.endswith('.json')]\n",
        "\n",
        "  for json_file in json_files:\n",
        "    file_path = year_path + '/{}'.format(json_file)\n",
        "    print (file_path)\n",
        "    gensim_process(file_path)\n",
        "    corpus_external.extend(corpus)\n",
        "    \n",
        "    dictionary_external.merge_with(dictionary)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "   \n",
        "\n",
        "print('\\n Corpus and Dictionary created\\n')\n",
        "print(dictionary_external)\n",
        "print(time.time()- start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter main folder to run: ANJO\n",
            "['1818', '1817', '1819', '1820', '1821', '1822', '1823', '1824', '1825', '1858', '1859', '1860']\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1818/02_11.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1817/07_30.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1819/03_17.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1819/06_30.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1820/07_26.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1820/11_29.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1820/06_07.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1820/08_16.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/05_16.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/05_30.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/07_04.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/06_27.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/09_26.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/03_07.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/12_19.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/12_05.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/09_12.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/02_07.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/06_06.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/12_12.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1821/01_31.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1822/07_31.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1822/02_06.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1822/07_10.json\n",
            "/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/ANJO/1822/04_24.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-072fa10500d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mgensim_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcorpus_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f99d0320cd50>\u001b[0m in \u001b[0;36mgensim_process\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mglobal\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \"\"\"\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oMY1bHUWybR"
      },
      "source": [
        "\n",
        "start = time.time()\n",
        "\n",
        "#initialize the tfidf model \n",
        "tfidf = models.TfidfModel(corpus_external)\n",
        "\n",
        "#apply the transofmration to the entire corpus \n",
        "transformed_tfidtf = tfidf[corpus_external]\n",
        "\n",
        "#use the LDA transformation on tfidf\n",
        "lda = models.LdaMulticore(transformed_tfidtf, num_topics=8, id2word=dictionary_external)\n",
        "lda.show_topics()\n",
        "\n",
        "#visualizing the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda, corpus_external, dictionary_external)\n",
        "\n",
        "print(time.time()- start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuyIRH6ZW0LO"
      },
      "source": [
        "#Run this for visualization\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcZiZwrPW1XH"
      },
      "source": [
        "#extract the visualization data \n",
        "with open('/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/vis_file_{}.html'.format(main_folder, main_folder), 'w') as file: \n",
        "    pyLDAvis.save_html(vis, file)\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfB2NL7DW2PB"
      },
      "source": [
        "\n",
        "csvfile = open('/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/corpus_{}.csv'.format(main_folder, main_folder), 'w')\n",
        "csvwriter = csv.writer(csvfile)\n",
        "for item in corpus_external:\n",
        "    csvwriter.writerow(item)\n",
        "csvfile.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70imfh5xW3Kn"
      },
      "source": [
        "w = csv.writer(open('/content/drive/My Drive/Cochrane Master Folder/Specific Articles Related to Cochrane/{}/dictionary_{}.csv'.format(main_folder, main_folder), \"w\"))\n",
        "for key, val in dictionary_external.items():\n",
        "  w.writerow([key, val])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}